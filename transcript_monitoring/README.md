# Transcript Monitoring System

Bu paket, Google Cloud Vertex AI ile entegre olarak transkript servislerinizi izlemek iÃ§in geliÅŸtirilmiÅŸ bir monitoring sistemidir. Decorator pattern ve context manager ile minimal kod deÄŸiÅŸikliÄŸi ile mevcut pipeline'Ä±nÄ±za seamless entegrasyon saÄŸlar.

## ğŸš€ Ã–zellikler

- **Seamless Integration**: Decorator pattern ile mevcut kodunuzu minimal deÄŸiÅŸtirerek monitoring aktif
- **Performance Tracking**: Request latency, audio processing time, model inference time
- **Quality Metrics**: Transcript uzunluÄŸu, confidence scores, token sayÄ±sÄ±
- **Business Analytics**: User bazÄ±nda usage tracking, cost monitoring
- **Background Processing**: Asenkron metrics gÃ¶nderimi (ana request'i yavaÅŸlatmaz)
- **Circuit Breaker**: Vertex AI down olsa bile servis Ã§alÄ±ÅŸmaya devam eder
- **Offline Mode**: Metrics offline kaydedilip sonra gÃ¶nderilebilir
- **Auto-retry**: Failed requests iÃ§in exponential backoff ile retry

## ğŸ“¦ Kurulum

```bash
# Monitoring dependencies ile birlikte
poetry install --extras monitoring

# Sadece core dependencies
poetry install
```

## âš™ï¸ YapÄ±landÄ±rma

`.env` dosyanÄ±zÄ± gÃ¼ncelleyin:

```env
# Vertex AI Configuration
VERTEX_AI_PROJECT_ID=your-gcp-project-id
VERTEX_AI_LOCATION=us-central1
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json

# Monitoring Settings
MONITORING_ENABLED=true
SEND_TO_VERTEX=true
MONITORING_BATCH_SIZE=10
```

## ğŸ¯ KullanÄ±m Ã–rnekleri

### 1. Decorator Pattern (Ã–nerilen)

```python
from transcript_monitoring import transcript_monitor

@transcript_monitor(
    user_context=lambda *args, **kwargs: kwargs.get('user_id'),
    track_audio_metadata=True,
    send_to_vertex=True
)
async def transcribe_audio(audio_data: bytes, user_id: str) -> dict:
    # Mevcut transkript kodunuz - hiÃ§bir ÅŸey deÄŸiÅŸmiyor!
    result = await gemini_api.transcribe(audio_data)
    return {
        "transcript": result.text,
        "confidence": result.confidence
    }

# KullanÄ±m
result = await transcribe_audio(audio_file, user_id="user123")
```

### 2. Context Manager Pattern

```python
from transcript_monitoring import TranscriptMonitor

async def advanced_transcribe(audio_data: bytes, user_id: str):
    async with TranscriptMonitor(user_id=user_id) as monitor:
        # Fine-grained control
        monitor.mark_audio_processing_start()

        # Audio preprocessing
        processed = await preprocess_audio(audio_data)
        monitor.add_audio_metadata(
            duration_seconds=30.5,
            format="wav",
            file_size_bytes=len(audio_data)
        )

        monitor.mark_audio_processing_end()
        monitor.mark_model_inference_start()

        # Gemini API call
        result = await gemini_api.transcribe(processed)

        monitor.mark_model_inference_end()
        monitor.set_result(result)

        return result
```

### 3. Mevcut Pipeline'Ä±nÄ±za Entegrasyon

```python
# Sadece decorator ekleyerek mevcut kodunuzu monitor edin
@transcript_monitor(
    user_context=lambda audio, user, **kw: user,
    track_audio_metadata=True
)
async def your_existing_function(audio_file: bytes, user_id: str):
    # Mevcut pipeline kodunuz - deÄŸiÅŸmiyor
    step1 = await diarization_step(audio_file)
    step2 = await transcription_step(step1)
    step3 = await post_processing_step(step2)
    return step3
```

## ğŸ“Š Metrics ve Analytics

### Performance Metrics
- **Total Duration**: Request baÅŸÄ±ndan sonuna kadar geÃ§en sÃ¼re
- **Audio Processing Time**: Audio preprocessing sÃ¼resi
- **Model Inference Time**: Gemini API Ã§aÄŸrÄ±sÄ± sÃ¼resi
- **Pipeline Duration**: Toplam pipeline sÃ¼resi

### Quality Metrics
- **Transcript Length**: Transcript uzunluÄŸu (karakter)
- **Word Count**: Kelime sayÄ±sÄ±
- **Confidence Scores**: Gemini'den gelen confidence deÄŸerleri
- **Average Confidence**: Ortalama confidence skoru

### Business Metrics
- **User Analytics**: User bazÄ±nda kullanÄ±m istatistikleri
- **Session Tracking**: Session bazÄ±nda performance
- **API Call Counts**: API Ã§aÄŸrÄ± sayÄ±larÄ±
- **Cost Tracking**: Maliyet takibi

## ğŸ”§ Advanced Configuration

### Custom Configuration

```python
from transcript_monitoring import transcript_monitor

@transcript_monitor(
    project_id="my-custom-project",
    user_context=lambda *args, **kwargs: extract_user_from_jwt(kwargs.get('token')),
    custom_config={
        "batch_size": 5,
        "send_to_vertex": True,
        "offline_mode": False
    }
)
async def my_function():
    pass
```

### Error Handling

```python
@transcript_monitor(user_context=lambda **kw: kw.get('user_id'))
async def safe_transcribe(audio_data: bytes, user_id: str):
    try:
        if not audio_data:
            raise ValueError("Audio required")

        return await process_audio(audio_data)
    except Exception as e:
        # Hata otomatik olarak metrics'e kaydedilir
        raise
```

## ğŸ“ˆ Dashboard ve Monitoring

Vertex AI Console'da aÅŸaÄŸÄ±daki metrikleri gÃ¶rÃ¼ntÃ¼leyebilirsiniz:

- **Real-time Performance**: Latency, throughput graphs
- **Quality Trends**: Confidence score trends over time
- **User Analytics**: User baÅŸÄ±na usage patterns
- **Error Monitoring**: Error rates ve types
- **Cost Analytics**: API usage ve cost optimization

## ğŸ› ï¸ Maintenance

### Health Check

```python
from transcript_monitoring import health_check, get_monitoring_stats

# Health check
status = await health_check()
print(f"System healthy: {status['healthy']}")

# Statistics
stats = get_monitoring_stats()
print(f"Metrics queued: {stats['background_sender']['queue_size']}")
```

### Graceful Shutdown

```python
from transcript_monitoring import stop_monitoring

# Uygulama kapatÄ±lÄ±rken
await stop_monitoring()  # Pending metrics'leri gÃ¶nderir
```

## ğŸ”’ Security ve Privacy

- **No Data Storage**: Audio data asla saklanmaz, sadece metadata
- **Secure Transmission**: TLS ile gÃ¼venli Vertex AI iletiÅŸimi
- **Configurable Privacy**: Hangi metriklerin toplanacaÄŸÄ±nÄ± kontrol edebilirsiniz
- **Offline Mode**: Ä°nternet baÄŸlantÄ±sÄ± olmasa bile Ã§alÄ±ÅŸÄ±r

## ğŸš¨ Troubleshooting

### Common Issues

1. **Vertex AI Connection Errors**
   ```bash
   # Check credentials
   gcloud auth application-default login
   export GOOGLE_APPLICATION_CREDENTIALS="/path/to/key.json"
   ```

2. **Circuit Breaker OPEN**
   ```python
   # Check Vertex AI status
   status = await health_check()
   print(status['vertex_ai'])  # Should be True
   ```

3. **Missing Dependencies**
   ```bash
   poetry install --extras monitoring
   ```

## ğŸ“‹ Best Practices

1. **Use Decorator Pattern** for new functions
2. **Use Context Manager** for fine-grained control
3. **Enable Offline Mode** during development
4. **Monitor Queue Sizes** in production
5. **Set Appropriate Batch Sizes** based on traffic
6. **Use Circuit Breaker** for resilience

## ğŸ Integration Examples

Projenizde `examples/monitoring_examples.py` dosyasÄ±na bakarak detaylÄ± kullanÄ±m Ã¶rneklerini inceleyebilirsiniz.

## ğŸ“ Support

- **Issues**: GitHub issues aÃ§abilirsiniz
- **Documentation**: Bu README ve kod iÃ§i docstring'ler
- **Examples**: `examples/` klasÃ¶rÃ¼ndeki Ã¶rnekler